{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73eeee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import traceback\n",
    "import os\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# LangChain Local Providers\n",
    "# from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d85ca25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is required to run LangGraph's async loops in a Jupyter Notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ee0298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvalCase:\n",
    "    query: str\n",
    "    expected_path: List[str]\n",
    "    required_keywords: List[str]\n",
    "\n",
    "@dataclass\n",
    "class PerformanceMetrics:\n",
    "    query: str\n",
    "    success: bool = False\n",
    "    total_steps: int = 0\n",
    "    path_taken: List[str] = field(default_factory=list)\n",
    "    loops_detected: int = 0\n",
    "    redundant_calls: int = 0\n",
    "    latency: float = 0.0\n",
    "    orchestration_error: bool = False\n",
    "    tokens_estimate: int = 0\n",
    "\n",
    "class NotebookEvaluator:\n",
    "    \"\"\"\n",
    "    Advanced Benchmark Suite supporting Local self-hosted APIs (Ollama).\n",
    "    Optimized for CPU-only environments.\n",
    "    \"\"\"\n",
    "    def __init__(self, workflow_factory, llm_list: Dict[str, Any]):\n",
    "        self.workflow_factory = workflow_factory\n",
    "        self.llms = llm_list\n",
    "        self.all_results = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_ollama_model(model_name: str):\n",
    "        \"\"\"\n",
    "        Connects to a local Ollama instance.\n",
    "        Enables JSON mode if supported by the model version.\n",
    "        \"\"\"\n",
    "        print(f\"Connecting to local Ollama model: {model_name}...\")\n",
    "        # format=\"json\" tells Ollama to strictly follow JSON if the model supports it\n",
    "        return ChatOllama(model=model_name, temperature=0, format=\"json\")\n",
    "\n",
    "    def _detect_loops_and_redundancy(self, path: List[str]) -> (int, int):\n",
    "        loops = 0\n",
    "        redundancy = 0\n",
    "        for i in range(len(path) - 2):\n",
    "            if path[i] == path[i+2] and path[i+1] == \"Supervisor\":\n",
    "                loops += 1\n",
    "        \n",
    "        counts = {}\n",
    "        for node in path:\n",
    "            if node in [\"Filing Analyst\", \"Price Analyst\"]:\n",
    "                counts[node] = counts.get(node, 0) + 1\n",
    "        \n",
    "        redundancy = sum(v - 1 for v in counts.values() if v > 1)\n",
    "        return loops, redundancy\n",
    "\n",
    "    async def run_benchmark(self, cases: List[EvalCase]):\n",
    "        try:\n",
    "            # We assume these are available in your environment\n",
    "            from crew import ContextSchema, AgentState\n",
    "        except ImportError:\n",
    "            from stockanalyzer.crew import ContextSchema, AgentState\n",
    "\n",
    "        for model_name, llm in self.llms.items():\n",
    "            print(f\"\\n{'='*60}\\nü§ñ BENCHMARKING LOCAL MODEL: {model_name}\\n{'='*60}\")\n",
    "            model_results = []\n",
    "            \n",
    "            # Note: If your workflow_factory (create_agent) uses .with_structured_output internally,\n",
    "            # you may need to modify crew.py to handle a simple chain + JsonOutputParser \n",
    "            # as a fallback for Ollama models.\n",
    "            workflow = self.workflow_factory()\n",
    "\n",
    "            for case in cases:\n",
    "                metrics = PerformanceMetrics(query=case.query)\n",
    "                start_time = time.time()\n",
    "                print(f\"\\nüìù Case: {case.query}\")\n",
    "                \n",
    "                try:\n",
    "                    context_obj = ContextSchema(model=llm)\n",
    "                    initial_state = AgentState(\n",
    "                        messages=[HumanMessage(content=case.query)],\n",
    "                        iteration_count=0\n",
    "                    )\n",
    "\n",
    "                    async for chunk in workflow.astream(\n",
    "                        initial_state,\n",
    "                        context=context_obj,\n",
    "                        stream_mode=\"updates\"\n",
    "                    ):\n",
    "                        for node_name, _ in chunk.items():\n",
    "                            node_str = str(node_name).split('.')[-1] if '.' in str(node_name) else str(node_name)\n",
    "                            metrics.path_taken.append(node_str)\n",
    "                            metrics.total_steps += 1\n",
    "                            print(f\"  ‚ûú {node_str}\", end=\"\", flush=True)\n",
    "\n",
    "                    metrics.latency = round(time.time() - start_time, 2)\n",
    "                    loops, redundancy = self._detect_loops_and_redundancy(metrics.path_taken)\n",
    "                    metrics.loops_detected = loops\n",
    "                    metrics.redundant_calls = redundancy\n",
    "                    \n",
    "                    first_worker = next((n for n in metrics.path_taken if n in [\"Filing Analyst\", \"Price Analyst\"]), None)\n",
    "                    expected_worker = next((n for n in case.expected_path if n in [\"Filing Analyst\", \"Price Analyst\"]), None)\n",
    "                    \n",
    "                    if expected_worker and first_worker != expected_worker:\n",
    "                        metrics.orchestration_error = True\n",
    "                    \n",
    "                    metrics.success = not metrics.orchestration_error and all(n in metrics.path_taken for n in case.expected_path)\n",
    "                    model_results.append(metrics)\n",
    "                    status = \"‚úÖ PASS\" if metrics.success else \"‚ùå FAIL\"\n",
    "                    print(f\"\\n  Result: {status} | Time: {metrics.latency}s | Steps: {metrics.total_steps}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    # Provide clearer guidance for the common Ollama error\n",
    "                    if \"with_structured_output\" in str(e):\n",
    "                        print(f\"\\n  ‚ùå Config Error: Local model '{model_name}' does not support with_structured_output.\")\n",
    "                        print(f\"     TIP: Update your supervisor_node in crew.py to use a JsonOutputParser fallback.\")\n",
    "                    else:\n",
    "                        print(f\"\\n  ‚ùå Execution Error for {model_name}: {e}\")\n",
    "\n",
    "            self.all_results[model_name] = model_results\n",
    "\n",
    "    def print_comparative_summary(self):\n",
    "        print(f\"\\n\\n{'#'*60}\\nüìä FINAL COMPARATIVE SUMMARY\\n{'#'*60}\")\n",
    "        header = f\"{'Model':<30} | {'Pass%':<8} | {'Avg Steps':<10} | {'Loops':<6} | {'Avg Latency':<10}\"\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "        \n",
    "        for model, results in self.all_results.items():\n",
    "            if not results: continue\n",
    "            total = len(results)\n",
    "            pass_rate = (sum(1 for r in results if r.success) / total) * 100\n",
    "            avg_steps = sum(r.total_steps for r in results) / total\n",
    "            total_loops = sum(r.loops_detected for r in results)\n",
    "            avg_latency = sum(r.latency for r in results) / total\n",
    "            \n",
    "            print(f\"{model:<30} | {pass_rate:>6.1f}% | {avg_steps:>10.1f} | {total_loops:>6} | {avg_latency:>10.1f}s\")\n",
    "        print(\"#\" * 60)\n",
    "\n",
    "async def run_local_benchmark():\n",
    "    # Attempting to import from the path where you likely have it\n",
    "    try:\n",
    "        from stockanalyzer.crew import create_agent\n",
    "    except ImportError:\n",
    "        # Fallback if the module structure is different\n",
    "        def create_agent(): raise ImportError(\"Please ensure crew.py is in your PYTHONPATH\")\n",
    "    \n",
    "    models_to_test = [\"qwen2.5:1.5b\", \"phi3.5:latest\"]\n",
    "    llm_bench = {}\n",
    "    \n",
    "    for model_id in models_to_test:\n",
    "        try:\n",
    "            llm_bench[model_id] = NotebookEvaluator.get_ollama_model(model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to Ollama for {model_id}: {e}\")\n",
    "\n",
    "    cases = [\n",
    "        EvalCase(\n",
    "            query=\"What was NVIDIA's revenue in their latest 10-Q?\",\n",
    "            expected_path=[\"Filing Analyst\", \"Synthesizer\"],\n",
    "            required_keywords=[\"NVIDIA\", \"Revenue\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    evaluator = NotebookEvaluator(create_agent, llm_bench)\n",
    "    await evaluator.run_benchmark(cases)\n",
    "    evaluator.print_comparative_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55afe3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_local_benchmark():\n",
    "    # Attempting to import from the path where you likely have it\n",
    "    try:\n",
    "        from stockanalyzer.crew import create_agent\n",
    "    except ImportError:\n",
    "        # Fallback if the module structure is different\n",
    "        def create_agent(): raise ImportError(\"Please ensure crew.py is in your PYTHONPATH\")\n",
    "    \n",
    "    models_to_test = [\"qwen2.5:1.5b\", \"phi3.5:latest\"]\n",
    "    llm_bench = {}\n",
    "    \n",
    "    for model_id in models_to_test:\n",
    "        try:\n",
    "            llm_bench[model_id] = NotebookEvaluator.get_ollama_model(model_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to Ollama for {model_id}: {e}\")\n",
    "\n",
    "    cases = [\n",
    "        EvalCase(\n",
    "            query=\"What was NVIDIA's revenue in their latest 10-Q?\",\n",
    "            expected_path=[\"Filing Analyst\", \"Synthesizer\"],\n",
    "            required_keywords=[\"NVIDIA\", \"Revenue\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    evaluator = NotebookEvaluator(create_agent, llm_bench)\n",
    "    await evaluator.run_benchmark(cases)\n",
    "    evaluator.print_comparative_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38d66024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to local Ollama model: qwen2.5:1.5b...\n",
      "Connecting to local Ollama model: phi3.5:latest...\n",
      "\n",
      "============================================================\n",
      "ü§ñ BENCHMARKING LOCAL MODEL: qwen2.5:1.5b\n",
      "============================================================\n",
      "\n",
      "üìù Case: What was NVIDIA's revenue in their latest 10-Q?\n",
      "  ‚ûú Supervisor  ‚ûú Synthesizer\n",
      "  Result: ‚ùå FAIL | Time: 27.87s | Steps: 2\n",
      "\n",
      "============================================================\n",
      "ü§ñ BENCHMARKING LOCAL MODEL: phi3.5:latest\n",
      "============================================================\n",
      "\n",
      "üìù Case: What was NVIDIA's revenue in their latest 10-Q?\n",
      "  ‚ûú Supervisor  ‚ûú Synthesizer\n",
      "  Result: ‚ùå FAIL | Time: 60.48s | Steps: 2\n",
      "\n",
      "\n",
      "############################################################\n",
      "üìä FINAL COMPARATIVE SUMMARY\n",
      "############################################################\n",
      "Model                          | Pass%    | Avg Steps  | Loops  | Avg Latency\n",
      "-----------------------------------------------------------------------------\n",
      "qwen2.5:1.5b                   |    0.0% |        2.0 |      0 |       27.9s\n",
      "phi3.5:latest                  |    0.0% |        2.0 |      0 |       60.5s\n",
      "############################################################\n"
     ]
    }
   ],
   "source": [
    "await run_local_benchmark()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
